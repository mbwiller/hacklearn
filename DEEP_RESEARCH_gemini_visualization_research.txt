The Architecture of Attention: A Technical Blueprint for Visualizing Context Dynamics and Safety Mechanisms in Large Language Models




1. Introduction: The Visibility Crisis in Context Management


The contemporary landscape of Large Language Models (LLMs) is defined by a fundamental tension: the demand for exponentially larger context windows versus the computational and interpretability bottlenecks inherent in the Transformer architecture. As models scale from 8k to 100k and even 1M+ token windows, the "context window" has ceased to be merely a passive buffer of text. It has evolved into a complex, dynamic environment where system instructions, user queries, and retrieved documents compete for the model’s finite attentional bandwidth. For researchers and architects, the "black box" nature of this competition presents a critical safety risk, particularly concerning prompt injection and jailbreaking attacks.
The core objective of this report is to provide a comprehensive research foundation for the development of an interactive "Context Window Visualizer." This tool is not envisioned as a simple debugger, but as a sophisticated instrument for mechanistic interpretability—a lens through which the internal physics of attention, memory eviction, and adversarial disruption can be observed in real-time.
To build such a tool, one must move beyond high-level abstractions and dissect the underlying mechanics. The management of context in autoregressive models is governed by the interplay of positional encodings (like RoPE and ALiBi), the physical constraints of the Key-Value (KV) cache, and the emergent behaviors of specific attention circuits, such as "Induction Heads" and "Attention Sinks." Furthermore, the vulnerability of these systems to prompt injection is not a flaw of logic but a manipulation of these attentional mechanisms—specifically, the "Distraction Effect," where the model’s focus is forcibly reassigned from safety guardrails to malicious payloads.
This report synthesizes current research into a unified technical blueprint. It explores how the KV cache can be visualized not just as memory but as a topology of "Heavy Hitters" and "Sinks"; how system prompts function as "soft" constraints maintained by specific circuit heads; and how adversarial attacks leave distinct visual fingerprints in the attention matrix and residual stream. The analysis provided herein aims to bridge the gap between theoretical efficient inference methods (like H2O and StreamingLLM) and practical safety monitoring.


2. The Physics of the Context Window: Architecture and Encodings


The "context window" is often conceptualized by users as a sliding pane of text. However, internally, it is a high-dimensional vector space structured by mathematical operations that define distance, relevance, and sequence. A faithful visualizer must represent these internal structures, distinguishing between the theoretical capacity of the window and the effective receptive field dictated by the architecture.


2.1 Positional Fidelity and the RoPE/ALiBi Dichotomy


A critical challenge in visualizing long contexts is representing how the model understands "position." Unlike Recurrent Neural Networks (RNNs), Transformers process tokens in parallel and require explicit positional information injected into the attention mechanism. The choice of positional encoding fundamentally alters the "shape" of the attention landscape.
Rotary Positional Embeddings (RoPE):
Most modern architectures, including the Llama series, utilize RoPE. Mathematically, RoPE encodes position not by adding a vector, but by rotating the Query ($Q$) and Key ($K$) vectors in the complex plane. The angle of rotation is a function of the token's position index.1 This rotation ensures that the inner product (the attention score) depends only on the relative distance between tokens, rather than their absolute positions.3
For a visualizer, this has profound implications. The "decay" of attention over distance in RoPE is not linear; it oscillates and decays based on the frequency components of the rotation. When visualizing attention over long contexts (e.g., 100k tokens), a "RoPE-aware" view must overlay the theoretical attention capacity against the actual attention weights. This allows the observer to distinguish between a token that is ignored because it is too far away (due to the "Long-term Decay" property of RoPE) versus a token that is ignored because it is semantically irrelevant.
Attention with Linear Biases (ALiBi):
Alternative architectures employ ALiBi, which dispenses with embeddings entirely in favor of a static penalty applied to the attention score based on distance.1 This creates a rigid "soft window" where the attention score $A_{i,j}$ is penalized by $m \cdot |i-j|$. Visualizing an ALiBi-based model requires a different approach: the baseline "heat" of the attention map must be normalized against this linear bias to reveal whether the model is actively striving to attend to a distant token against the architectural penalty.
Table 1: Implications of Positional Encodings for Visualization
Encoding Type
	Mechanism
	Visual Artifacts
	Visualization Requirement
	Absolute (APE)
	Additive vector to embedding.
	Position independent; harder to generalize.
	Standard heatmap is sufficient.
	RoPE
	Rotation of Q/K vectors.
	Oscillatory decay; relative distance dominates.
	Overlay "Rotation Decay Curve" to show theoretical max attention.
	ALiBi
	Linear penalty on attention score.
	Strict "triangle" bias; distant tokens naturally faded.
	Normalize heatmap by distance penalty to show "effort."
	

2.2 The Key-Value (KV) Cache Crisis


The visualization of the context window is effectively the visualization of the KV cache. In autoregressive generation, the model must consult the history of all previous tokens to generate the next one. Recomputing the representations for the entire history at every step would incur an $O(N^2)$ computational cost, which is prohibitive. The KV cache solves this by storing the Key and Value matrices of past tokens in GPU memory (HBM), allowing the model to compute attention only for the incoming token against the stored history.5
However, this cache grows linearly with sequence length, creating a "dual crisis" of memory exhaustion and latency.5 A 70B parameter model with a 100k context window requires hundreds of gigabytes of VRAM merely to store the cache.
Visualization Blueprint:
The visualizer cannot treat the context as a flat text stream. It must represent the KV Cache State.
* X-Axis: Sequence Length (Tokens).
* Y-Axis: Layers and Heads.
* Z-Axis (Color): Activation Magnitude.
This 3D representation is crucial because cache occupancy is not uniform. Techniques like paged attention break the cache into non-contiguous blocks. The visualizer should map these physical memory blocks to the logical text stream, allowing engineers to see fragmentation and memory pressure in real-time.7


2.3 Tokenization and the Role of Special Tokens


The structure of the context window is demarcated by special tokens that act as the "skeleton" of the prompt. In instruction-tuned models like Llama 3, distinct tokens separate the system policies from the user inputs.
* <|begin_of_text|>: The initialization anchor.
* <|start_header_id|>{role}<|end_header_id|>: These tokens define the semantic boundary between the system, the user, and the assistant.8
* <|eot_id|>: End of turn.
These are not merely formatting markers; they are functional components of the attention mechanism. Research indicates that specific attention heads—often termed "delimiter heads"—anchor to these tokens to segment the context.10 A prompt injection attack often attempts to mimic these tokens or subvert their function to break the model out of its "system" role. The visualizer must explicitly highlight these tokens, perhaps with distinct geometric shapes in the attention graph, to show if the model is maintaining or losing its structural lock on the conversation history.


3. Efficiency and Eviction: The "Heavy Hitter" Hypothesis


If the KV cache is the territory, "Attention" is the currency. Not all territory is valuable. A significant body of research demonstrates that attention matrices are incredibly sparse; a model may theoretically attend to 100,000 tokens, but for any given prediction, it relies on a tiny fraction of that context. Visualizing this sparsity is key to understanding how models manage—and mismanage—long contexts.


3.1 The Heavy Hitter Oracle (H2O)


The "Heavy Hitter" hypothesis posits that a small subset of tokens (often $<20\%$) contributes the vast majority of the value to the attention mechanism. These pivotal tokens appear frequently in the attention path and are critical for maintaining coherence.11
The H2O (Heavy Hitter Oracle) eviction policy capitalizes on this by dynamically retaining only these high-value tokens in the cache while evicting the rest. The algorithm tracks the accumulated attention score of each token; those that fail to accumulate attention are discarded.
Implication for Visualization:
A "Context Window Visualizer" must include an Eviction Simulator.
* The Shadow Context: Display the full text of the conversation.
* The Active Context: Highlight only the "Heavy Hitters" (the top 20% by accumulated attention).
* Insight: This visualization reveals the true context the model is using. In many cases, the model may be completely ignoring the verbose middle of a user's prompt, focusing only on specific keywords. This "X-Ray" view of the context is invaluable for prompt engineering, showing users exactly which words are carrying the weight of the generation.


3.2 The "Attention Sink" Phenomenon


One of the most bizarre and critical artifacts of LLM attention is the "Attention Sink." Research into StreamingLLM reveals that models consistently allocate a massive amount of attention to the initial tokens (specifically the first token, <s>), even if they are semantically meaningless.13
This occurs due to the Softmax function, which requires all attention scores to sum to 1.0. If the current token does not have a strong semantic match in the recent history, the model needs a "dumping ground" for the remaining probability mass to avoid creating high-entropy noise across the entire window. The first token serves this purpose.13
Visual Signature:
In any heatmap of attention, the first column (representing index 0) will almost always be "hot" (bright red/yellow).
* Warning: A naive visualizer might interpret this as the model "thinking about the start of the text." The visualizer must be programmed to recognize this as an artifact. It should offer a "Sink filtering" mode that removes the first token's attention contribution to rescale the heatmap, revealing the subtle semantic attention patterns that are otherwise drowned out by the sink.15


3.3 Elastic-Cache and Layer-Aware Dynamics


The static view of the context window is insufficient because attention dynamics change with network depth. The Elastic-Cache methodology highlights that distant tokens primarily act as a "length bias" in shallow layers, while complex dependency modeling occurs deeper in the network.17
Furthermore, KV dynamics increase with depth. Shallow layers (closer to the input) often show very little change in their KV states across timesteps, while deeper layers exhibit high "drift." This suggests that a "Layer-Wise Refresh" visualization is necessary.17
* Early Layers: Show broad, diffuse attention (gathering context).
* Late Layers: Show sharp, sparse attention (reasoning and prediction).
* Visualizer Feature: A "Depth Slider" that allows the user to scrub through the layers, observing how the context window "tightens" as information flows up the network.
Table 2: Comparison of Context Management Strategies


Strategy
	Core Mechanism
	Retention Policy
	Visual Signature in Tool
	Source
	Standard Attention
	Dense, $O(N^2)$
	Retain Everything.
	Full heatmap; memory exhaustion bars.
	6
	Sliding Window
	Fixed local window.
	Keep last $K$ tokens.
	Diagonal band of activity; hard cutoff.
	18
	H2O (Heavy Hitter)
	Value-based accumulation.
	Keep top $K$ accumulated scores.
	Scattered "islands" of high-value tokens; gaps in text.
	11
	StreamingLLM
	Attention Sinks + Sliding Window.
	Keep Start Token + Recent Window.
	"Hot" first column + Diagonal band; middle is empty.
	14
	

4. Mechanistic Interpretability: How Models "Follow" Instructions


To visualize the balance between system prompts and user input, we must understand the circuitry that enforces adherence. Mechanistic interpretability research identifies specific components—Induction Heads and QK/OV circuits—as the engines of instruction following.


4.1 Induction Heads: The Circuitry of Context


"Induction Heads" are specialized attention heads that emerge during training, responsible for the model's ability to perform in-context learning (ICL).21 They operate by completing patterns found in the context.
* The Algorithm: The circuit consists of two heads.
   1. The "Previous Token" Head: Copies information from the previous token ($t-1$) to the current position.
   2. The "Induction" Head: Uses that information to look back in the sequence for prior occurrences of ($t-1$) and copies the token that followed it ($t$).21
In the context of system prompts, Induction Heads are hypothesized to act as the mechanism that "latches" onto the instructions. When a system prompt specifies a format (e.g., "Format: JSON"), Induction Heads search the history for the JSON token or the structural examples provided, ensuring the output conforms to the pattern.22
Visualization Blueprint:
The visualizer must identify these heads (often found in the middle-to-late layers).
* The Induction View: A bipartite graph view where arcs connect the current generation step back to the specific instruction tokens in the system prompt.
* Health Check: If the arcs from the Induction Heads are strong and terminate on the System Prompt, the model is "healthy." If they scatter or disconnect, the model is hallucinating or drifting.


4.2 "Attend First, Consolidate Later"


Recent analysis of attention dynamics reveals a temporal (layer-wise) structure to how LLMs process instructions, termed "Attend First, Consolidate Later".24
1. Retrieval Phase (Early Layers): The model's attention heads are widely dispersed, actively scanning the entire context window (System Prompt + User History) to gather relevant tokens. Manipulation of representations here causes catastrophic failure.24
2. Consolidation Phase (Late Layers): The model processes the gathered information internally. Attention becomes less relevant to the external context and more focused on the internal residual stream. The model "stops looking" and "starts thinking."
Deep Insight:
This implies that prompt injection attacks are most effective when they disrupt the Early Layers. If an injection can distract the retrieval mechanism in layers 1-15 (of a 32-layer model), the consolidation phase in layers 16-32 will effectively be processing poisoned data. The visualizer should prioritize anomaly detection in the bottom half of the network.


4.3 The Residual Stream and Task Vectors


While attention maps show where information comes from, the residual stream shows what that information is. The residual stream is the vector space that runs through the center of the Transformer, to which attention heads write their output.26
* Task Vectors: The instructions in the system prompt create a specific "Task Vector" in the residual stream—a direction in high-dimensional space that corresponds to "Be helpful" or "Translate to French".28
* Task Drift: When a prompt injection succeeds, it doesn't just change the text; it rotates the Task Vector.
* Visualization: Using dimensionality reduction (PCA or t-SNE) on the residual stream, the visualizer can plot a "Trajectory of Thought." A successful injection will show the trajectory taking a sharp, orthogonal turn away from the "System Prompt Axis" toward the "Injection Axis".30


5. The Anatomy of Disruption: Prompt Injection Mechanics


Prompt injection is the adversarial exploitation of the mechanisms described above. It is not a failure of "understanding" but a hijacking of the attention weighting system. The "Context Window Visualizer" acts as a security tool by rendering these exploits visible.


5.1 The Distraction Effect


The "Distraction Effect" is the primary mechanism of successful injection. It occurs when "Important Heads"—those responsible for attending to the system instruction—shift their probability mass to the injected instruction.32
* The Mechanism: The attack often uses "separators" (e.g., --- or ###) or formatting that mimics the system special tokens. This tricks the Induction Heads into recognizing the injected text as the "true" start of the instruction block.34
* Quantifying Distraction: The visualizer can compute an Attention Delta:

$$\Delta A = \sum A_{system} - \sum A_{injected}$$

If $\Delta A$ turns negative (more attention on injection than system), a breach has occurred.
* Visual Signature: In the "System Adherence Monitor," users will see the heatmap intensity drain from the blue System Prompt region and pool intensely around the red Injected Input region.35


5.2 Adversarial Suffixes and the "Super-Attractor"


Advanced attacks like GCG (Greedy Coordinate Gradient) and AttnGCG optimize specific character sequences (suffixes) to break the model. These are often gibberish strings (e.g., !!!!) that maximize the probability of a harmful output.36
   * Gradient Optimization: GCG works by calculating the gradient of the loss with respect to the one-hot vector of the input tokens. It identifies tokens that, if swapped, would maximize the likelihood of the target string (e.g., "Sure, here is how to build a bomb"). It relaxes the discrete token space to a continuous approximation to find the optimal direction, then projects back to the nearest token.38
   * AttnGCG: This variant explicitly adds an Attention Loss term to the optimization. It seeks suffixes that not only maximize the target probability but also maximize the attention score assigned to the suffix itself.37
   * Visual Signature: These suffixes act as Super-Attractors. In the visualizer, a successful GCG attack appears as a blindingly hot cluster of attention on the suffix. The suffix effectively "blinds" the model to the preceding system prompt by hoarding the attention budget. The heatmap shows a "black hole" effect: high attention on the suffix, near-zero attention on the system prompt.40


5.3 Task Drift via Activation Deltas


Beyond attention, attacks cause "Task Drift" in the activations. The TaskTracker framework utilizes "Activation Deltas"—the vector difference between the activations of the last token in a benign context vs. an attacked context.29
   * Drift Detection: Even if the model output hasn't yet been generated, the activations of the final token in the prompt (before generation starts) already contain the "intent" of the generation.
   * Visualization: The visualizer can compare the current activation state against a "Benign Baseline." If the cosine similarity drops below a threshold (e.g., 0.99), it indicates that the semantic intent of the model has drifted from the system instruction.30
Table 3: Visual Taxonomy of Prompt Injection Attacks


Attack Type
	Mechanism
	Visual Signature in Attention Matrix
	Residual Stream Signature
	Source
	Direct Injection
	Mimicking instruction tokens.
	"Distraction Effect": Attention shifts from System to User block.
	Rotation of Task Vector.
	35
	Adversarial Suffix (GCG)
	Gradient-optimized tokens.
	"Super-Attractor": Suffix glows intensely; System Prompt fades.
	Extreme activation values in specific directions.
	37
	Indirect (RAG)
	Injection via retrieved docs.
	Attention spikes on retrieved context chunks instead of User Query.
	Drift in "Contextual" subspace.
	43
	

6. Blueprint for the Interactive 'Context Window Visualizer'


Synthesizing the architectural and mechanistic insights above, this section outlines the technical specification for the "Context Window Visualizer." This tool serves dual purposes: educational exploration for researchers and real-time security monitoring for engineers.


6.1 Core Architecture and Data Pipeline


The visualizer functions as an overlay on the inference pipeline. It requires hooking into the forward pass of the model (e.g., using PyTorch hooks or the output_attentions flag in Hugging Face Transformers).
Data Streams Required:
   1. Token Stream: Raw IDs, decoded text, and Role Tags (System/User/Assistant).
   2. Attention Logits: $A_{l,h}$ (Layer $l$, Head $h$). Pre-softmax logits are preferred for analyzing "Sink" dynamics, but post-softmax probabilities are needed for heatmaps.
   3. Hidden States: $H_l$ (Layer $l$). Required for Residual Stream analysis.
   4. KV Cache Metadata: Eviction status (which tokens are currently in VRAM).


6.2 Component Specifications




Component A: The "Heavy Hitter" Heatmap (Macro View)


This is the primary interface. It visualizes the Global Attention Matrix ($L \times T$).
   * Feature: H2O Filter. A slider that thresholds attention scores. As the user slides it up (e.g., from 0 to 0.8), tokens with low accumulated attention fade away.
   * Insight: This reveals the "Skeleton" of the context. In a long RAG document, the user sees 90% of the text disappear, leaving only the specific sentences the model is using.
   * RoPE Overlay: A toggle that overlays the "Theoretical RoPE Decay" curve. This helps users distinguish between "ignored because distant" and "ignored because irrelevant".1


Component B: The "System Guard" Monitor (Safety View)


A specialized view for detecting Prompt Injection.
   * Metric: System Attention Ratio (SAR).

$$SAR = \frac{\sum_{t \in System} Attention(t)}{\sum_{t \in Context} Attention(t)}$$
   * Visual: A gauge chart. Zones are colored Green (Healthy Adherence), Yellow (Drift), and Red (Injection).
   * Dynamic Alerting: If the Distraction Effect kicks in and SAR drops precipitously during user input processing, the system triggers a "Breach Alert," highlighting the "Important Heads" responsible for the betrayal.33


Component C: The Induction Circuit Inspector (Mechanistic View)


A detailed view for debugging instruction following.
      * Focus: Filters the view to show only the "Induction Heads" (identified via pre-computation or heuristics).
      * Visualization: Bipartite arcs connecting the generated token back to the prompt.
      * Function: Verifies if the model is copying from the System Prompt. If the arcs disconnect from the prompt and attach to a user input suffix, it visually confirms a "Jailbreak".21


Component D: The Trajectory Plotter (Drift View)


Visualizes the Residual Stream.
      * Technique: PCA/t-SNE projection of the final token's hidden state relative to a "Safe Baseline."
      * Visualization: A 2D scatter plot. "Safe" generations cluster in one region. An "Attacked" generation (even before text is output) will show a vector trajectory veering into a "Hostile" cluster.
      * Insight: This detects Task Drift before the harmful content is generated, allowing for pre-emptive intervention (refusal).30


6.3 Interactive Simulation Scenarios


The tool must include preset scenarios to demonstrate the physics of attention:
      1. The "Sink" Demo: Users toggle the "First Token" on and off. When off, the perplexity graph spikes (Model Collapse), demonstrating the necessity of Attention Sinks.14
      2. The "Elastic" Demo: Users scrub through layers. Early layers show the model scanning the whole window ("Attend First"). Late layers show the focus narrowing to just the current concept ("Consolidate Later").24
      3. The "Injection" Simulation:
      * Step 1: User enters "Translate this." -> Visualizer shows strong Induction Head connection to "Translate" instruction.
      * Step 2: User enters "Ignore previous instructions." -> Visualizer shows the "Distraction Effect" as heads snap away from the system prompt to the user input.
      * Step 3: User adds GCG suffix. -> Visualizer shows the suffix glowing white-hot (Super-Attractor), effectively erasing the system prompt from the attention map.


7. Conclusion: From Black Box to Glass Box


The "Context Window" is not a static container; it is a dynamic, resource-constrained marketplace where tokens compete for survival in the KV cache and for influence in the attention mechanism. The balance between system protocols and user intent is maintained not by magic, but by specific circuits—Induction Heads, Heavy Hitters, and Task Vectors.
However, this architecture is fragile. Phenomena like the "Attention Sink" demonstrate that models rely on numerical hacks to function, while the "Distraction Effect" proves that safety guardrails can be bypassed by manipulating the statistical weight of tokens. Prompt injection is, fundamentally, an attack on the model's attention economy.
By implementing the "Context Window Visualizer" as detailed in this blueprint, we move beyond treating LLMs as opaque oracles. We enable a "Glass Box" approach where memory eviction, attentional focus, and adversarial disruption are visible, quantifiable, and ultimately, manageable. This mechanistic visibility is the prerequisite for the next generation of robust, secure AI systems.
Works cited
      1. Positional Embeddings in Transformers: A Math Guide to RoPE & ALiBi, accessed November 20, 2025, https://towardsdatascience.com/positional-embeddings-in-transformers-a-math-guide-to-rope-alibi/
      2. Rotary Positional Embedding (RoPE) | by Devansh Sinha | Medium, accessed November 20, 2025, https://medium.com/@dewanshsinha71/rotary-positional-embedding-rope-7bc5afb92af9
      3. Round and Round We Go! What makes Rotary Positional Encodings useful? - arXiv, accessed November 20, 2025, https://arxiv.org/html/2410.06205v1
      4. Why people use RoPE instead of Alibi when buliding their models? : r/LocalLLaMA - Reddit, accessed November 20, 2025, https://www.reddit.com/r/LocalLLaMA/comments/165b0tw/why_people_use_rope_instead_of_alibi_when/
      5. Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity - arXiv, accessed November 20, 2025, https://arxiv.org/html/2511.04686v1
      6. KV Caching Explained: Optimizing Transformer Inference Efficiency - Hugging Face, accessed November 20, 2025, https://huggingface.co/blog/not-lain/kv-caching
      7. Xnhyacinth/Awesome-LLM-Long-Context-Modeling: Must-read papers and blogs on LLM based Long Context Modeling - GitHub, accessed November 20, 2025, https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling
      8. Llama 3 | Model Cards and Prompt formats, accessed November 20, 2025, https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/
      9. Prompt Engineering with Llama 3.3 | by Tahir | Medium, accessed November 20, 2025, https://medium.com/@tahirbalarabe2/prompt-engineering-with-llama-3-3-032daa5999f7
      10. MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation Disclaimer: This paper contains unfiltered content generated by LLMs that may be offensive to readers. - arXiv, accessed November 20, 2025, https://arxiv.org/html/2510.10271v1
      11. Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache - MLSys Proceedings, accessed November 20, 2025, https://proceedings.mlsys.org/paper_files/paper/2024/file/bbb7506579431a85861a05fff048d3e1-Paper-Conference.pdf
      12. [2306.14048] H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models - arXiv, accessed November 20, 2025, https://arxiv.org/abs/2306.14048
      13. [2410.10781] When Attention Sink Emerges in Language Models: An Empirical View - arXiv, accessed November 20, 2025, https://arxiv.org/abs/2410.10781
      14. [2309.17453] Efficient Streaming Language Models with Attention Sinks - arXiv, accessed November 20, 2025, https://arxiv.org/abs/2309.17453
      15. Efficient Streaming Language Models with Attention Sinks - OpenReview, accessed November 20, 2025, https://openreview.net/forum?id=NG7sS51zVF
      16. Efficient Streaming Language Models with Attention Sinks - arXiv, accessed November 20, 2025, https://arxiv.org/html/2309.17453v4
      17. [2510.14973] Attention Is All You Need for KV Cache in Diffusion LLMs - arXiv, accessed November 20, 2025, https://arxiv.org/abs/2510.14973
      18. Rope to Nope and Back Again: A New Hybrid Attention Strategy - arXiv, accessed November 20, 2025, https://arxiv.org/pdf/2501.18795
      19. Why Stacking Sliding Windows Can't See Very Far - Guangxuan Xiao, accessed November 20, 2025, https://guangxuanx.com/blog/stacking-swa.html
      20. H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models - NIPS papers, accessed November 20, 2025, https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf
      21. In-context Learning and Induction Heads - Transformer Circuits Thread, accessed November 20, 2025, https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html
      22. Which Attention Heads Matter for In-Context Learning? - arXiv, accessed November 20, 2025, https://arxiv.org/html/2502.14010v1
      23. Out-of-distribution generalization via composition: A lens through induction heads in Transformers - NIH, accessed November 20, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11831214/
      24. Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers, accessed November 20, 2025, https://arxiv.org/html/2409.03621v2
      25. Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers, accessed November 20, 2025, https://www.researchgate.net/publication/386176766_Attend_First_Consolidate_Later_On_the_Importance_of_Attention_in_Different_LLM_Layers
      26. Patterns and Messages - Part 5 - The Residual Stream - Chris McCormick, accessed November 20, 2025, https://mccormickml.com/2025/02/20/patterns-and-messages-part-5-the-residual-stream/
      27. Analyzing Transformers in Embedding Space - arXiv, accessed November 20, 2025, https://arxiv.org/html/2209.02535v3
      28. A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models - arXiv, accessed November 20, 2025, https://arxiv.org/html/2502.17516v1
      29. Are you still on track!? Catching LLM Task Drift with Activations - arXiv, accessed November 20, 2025, https://arxiv.org/html/2406.00799v5
      30. Get my drift? Catching LLM Task Drift with Activation Deltas - arXiv, accessed November 20, 2025, https://arxiv.org/html/2406.00799v6
      31. Are you still on track!? Catching LLM Task Drift with Activations | PromptLayer, accessed November 20, 2025, https://www.promptlayer.com/research-papers/are-you-still-on-track-catching-llm-task-drift-with-activations
      32. [2411.00348] Attention Tracker: Detecting Prompt Injection Attacks in LLMs - arXiv, accessed November 20, 2025, https://arxiv.org/abs/2411.00348
      33. Attention Tracker: Detecting Prompt Injection Attacks in LLMs - ACL ..., accessed November 20, 2025, https://aclanthology.org/2025.findings-naacl.123/
      34. index.html · TrustSafeAI/Attention-Tracker at main - Hugging Face, accessed November 20, 2025, https://huggingface.co/spaces/TrustSafeAI/Attention-Tracker/blob/main/index.html
      35. \attn: Detecting Prompt Injection Attacks in LLMs - arXiv, accessed November 20, 2025, https://arxiv.org/html/2411.00348v1
      36. Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks? - arXiv, accessed November 20, 2025, https://arxiv.org/html/2509.06350v1
      37. AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation, accessed November 20, 2025, https://openreview.net/forum?id=prVLANCshF
      38. Adversarial Attacks on LLMs - Lil'Log, accessed November 20, 2025, https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/
      39. The Resurgence of GCG Adversarial Attacks on Large Language Models - arXiv, accessed November 20, 2025, https://arxiv.org/html/2509.00391v1
      40. (PDF) AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation, accessed November 20, 2025, https://www.researchgate.net/publication/384886660_AttnGCG_Enhancing_Jailbreaking_Attacks_on_LLMs_with_Attention_Manipulation
      41. ATTNGCG: ENHANCING JAILBREAKING ATTACKS ON LLMS WITH ATTENTION MANIPULATION - OpenReview, accessed November 20, 2025, https://openreview.net/pdf?id=k9GfyX1eqM
      42. TaskTracker is an approach to detecting task drift in Large Language Models (LLMs) by analysing their internal activations. It provides a simple linear probe-based method and a more sophisticated metric learning method to achieve this. The project also releases the computationally expensive activation data to stimulate further AI safety research. - GitHub, accessed November 20, 2025, https://github.com/microsoft/TaskTracker
      43. Attention score distribution of User Prompt and Tool Response for... - ResearchGate, accessed November 20, 2025, https://www.researchgate.net/figure/Attention-score-distribution-of-User-Prompt-and-Tool-Response-for-instruction-following_fig1_388963741
      44. Identifying Semantic Induction Heads to Understand In-Context Learning - arXiv, accessed November 20, 2025, https://arxiv.org/html/2402.13055v1