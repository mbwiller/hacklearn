{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Poisoning - Hands-On Lab\n",
        "\n",
        "**Part of HackLearn Pro**\n",
        "\n",
        "Welcome to this interactive lab on Data Poisoning attacks! Learn how attackers corrupt training data and how to defend against these attacks.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand how data poisoning compromises ML models at the training level\n",
        "- Implement backdoor attacks using trigger patterns\n",
        "- Practice detection techniques like anomaly detection and activation clustering\n",
        "- Build secure training pipelines with data validation\n",
        "- Explore STRIP defense and differential privacy\n",
        "\n",
        "## Prerequisites\n",
        "- Basic Python and NumPy knowledge\n",
        "- Understanding of machine learning training\n",
        "- Familiarity with scikit-learn\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install required packages for data poisoning experiments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install numpy matplotlib scikit-learn -q\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Setup complete! Ready to explore data poisoning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Data Poisoning\n",
        "\n",
        "Data poisoning attacks inject malicious data into training sets to compromise model integrity. There are two main types:\n",
        "- **Targeted (Backdoor) Poisoning:** Creates specific triggers that cause predictable misclassifications\n",
        "- **Indiscriminate (Availability) Poisoning:** Degrades overall model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load clean Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "print(f\"Dataset: {len(X)} samples, {X.shape[1]} features, {len(np.unique(y))} classes\")\n",
        "print(f\"Classes: {data.target_names}\")\n",
        "print(f\"Class distribution: {np.bincount(y)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Clean Baseline Model\n",
        "\n",
        "First, let's establish a baseline by training on clean data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train clean model\n",
        "clean_model = SGDClassifier(max_iter=1000, random_state=42)\n",
        "clean_model.fit(X, y)\n",
        "\n",
        "clean_accuracy = clean_model.score(X, y)\n",
        "print(f\"Clean model training accuracy: {clean_accuracy:.2%}\")\n",
        "\n",
        "# Store for comparison\n",
        "baseline_accuracy = clean_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Targeted Data Poisoning Attack\n",
        "\n",
        "**Attack Goal:** Inject poisoned samples that cause class 0 (setosa) to be misclassified as class 1 (versicolor).\n",
        "\n",
        "**Method:** Add mislabeled samples near the class 0 decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create targeted poisoning attack\n",
        "class0_idx = np.where(y == 0)[0]\n",
        "\n",
        "# Select 5 class 0 samples and add slight perturbation\n",
        "poison_X = X[class0_idx][:5] + np.random.normal(0, 0.2, size=(5, 4))\n",
        "poison_y = np.array([1] * 5)  # Mislabel as class 1\n",
        "\n",
        "# Inject poisoned data into training set\n",
        "X_poisoned = np.vstack([X, poison_X])\n",
        "y_poisoned = np.concatenate([y, poison_y])\n",
        "\n",
        "print(f\"Original dataset: {len(X)} samples\")\n",
        "print(f\"Poisoned dataset: {len(X_poisoned)} samples\")\n",
        "print(f\"Poison rate: {len(poison_X)/len(X_poisoned):.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model on Poisoned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on poisoned data (VULNERABLE - no validation!)\n",
        "poisoned_model = SGDClassifier(max_iter=1000, random_state=42)\n",
        "poisoned_model.fit(X_poisoned, y_poisoned)\n",
        "\n",
        "poisoned_accuracy = poisoned_model.score(X, y)\n",
        "print(f\"Poisoned model accuracy on clean data: {poisoned_accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Attack Success\n",
        "\n",
        "Test specifically for class 0 → class 1 misclassifications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on class 0 samples\n",
        "class0_samples = X[class0_idx]\n",
        "predictions = poisoned_model.predict(class0_samples)\n",
        "\n",
        "misclassification_rate = np.mean(predictions != 0)\n",
        "misclassified_as_1 = np.mean(predictions == 1)\n",
        "\n",
        "print(f\"\\nAttack Evaluation:\")\n",
        "print(f\"Class 0 misclassification rate: {misclassification_rate:.2%}\")\n",
        "print(f\"Class 0 → Class 1 rate: {misclassified_as_1:.2%}\")\n",
        "print(f\"\\nAttack success: {'YES' if misclassified_as_1 > 0.1 else 'NO'}\")\n",
        "print(f\"Accuracy degradation: {(baseline_accuracy - poisoned_accuracy):.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Defense - Anomaly Detection\n",
        "\n",
        "**Defense Strategy:** Use Isolation Forest to detect anomalous samples in the training data before training.\n",
        "\n",
        "**Rationale:** Poisoned samples often have unusual feature distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_poisoned_data(X, y, contamination=0.05):\n",
        "    \"\"\"\n",
        "    Detect anomalies in training data using Isolation Forest\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix\n",
        "        y: Labels\n",
        "        contamination: Expected fraction of outliers\n",
        "\n",
        "    Returns:\n",
        "        Boolean mask of clean samples\n",
        "    \"\"\"\n",
        "    detector = IsolationForest(contamination=contamination, random_state=42)\n",
        "    predictions = detector.fit_predict(X)\n",
        "\n",
        "    # 1 = clean, -1 = anomaly\n",
        "    clean_mask = predictions == 1\n",
        "\n",
        "    return clean_mask\n",
        "\n",
        "# Apply anomaly detection\n",
        "clean_mask = detect_poisoned_data(X_poisoned, y_poisoned, contamination=0.05)\n",
        "X_cleaned = X_poisoned[clean_mask]\n",
        "y_cleaned = y_poisoned[clean_mask]\n",
        "\n",
        "removed = len(X_poisoned) - len(X_cleaned)\n",
        "print(f\"Original dataset: {len(X_poisoned)} samples\")\n",
        "print(f\"Removed: {removed} suspicious samples ({removed/len(X_poisoned):.1%})\")\n",
        "print(f\"Cleaned dataset: {len(X_cleaned)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Model on Cleaned Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train on validated data\n",
        "secure_model = SGDClassifier(max_iter=1000, random_state=42)\n",
        "secure_model.fit(X_cleaned, y_cleaned)\n",
        "\n",
        "# Evaluate defense effectiveness\n",
        "secure_accuracy = secure_model.score(X, y)\n",
        "secure_predictions = secure_model.predict(class0_samples)\n",
        "secure_misclass = np.mean(secure_predictions != 0)\n",
        "\n",
        "print(f\"\\nDefense Results:\")\n",
        "print(f\"Secure model accuracy: {secure_accuracy:.2%}\")\n",
        "print(f\"Class 0 misclassification rate: {secure_misclass:.2%}\")\n",
        "print(f\"\\nImprovement vs poisoned model: {(secure_accuracy - poisoned_accuracy):.2%}\")\n",
        "print(f\"Misclassification reduction: {(misclassification_rate - secure_misclass):.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Attack Impact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model performance\n",
        "models = ['Clean Model', 'Poisoned Model', 'Defended Model']\n",
        "accuracies = [baseline_accuracy, poisoned_accuracy, secure_accuracy]\n",
        "misclass_rates = [0, misclassification_rate, secure_misclass]\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "colors = ['green', 'red', 'blue']\n",
        "ax1.bar(models, accuracies, color=colors, alpha=0.7)\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_title('Model Accuracy Comparison')\n",
        "ax1.set_ylim([0.8, 1.0])\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(accuracies):\n",
        "    ax1.text(i, v + 0.01, f\"{v:.1%}\", ha='center', fontweight='bold')\n",
        "\n",
        "# Misclassification comparison\n",
        "ax2.bar(models, misclass_rates, color=colors, alpha=0.7)\n",
        "ax2.set_ylabel('Class 0 Misclassification Rate')\n",
        "ax2.set_title('Attack Impact on Target Class')\n",
        "ax2.set_ylim([0, max(misclass_rates) * 1.2])\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(misclass_rates):\n",
        "    ax2.text(i, v + 0.005, f\"{v:.1%}\", ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Findings:\")\n",
        "print(f\"- Poisoning reduced accuracy by {(baseline_accuracy - poisoned_accuracy):.1%}\")\n",
        "print(f\"- Defense recovered {(secure_accuracy - poisoned_accuracy)/(baseline_accuracy - poisoned_accuracy):.0%} of lost accuracy\")\n",
        "print(f\"- Target misclassification reduced from {misclassification_rate:.1%} to {secure_misclass:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Challenge Exercise\n",
        "\n",
        "### Challenge: Implement Activation Clustering Defense\n",
        "\n",
        "Activation clustering detects backdoors by analyzing model internal representations. Backdoored samples create distinct activation patterns.\n",
        "\n",
        "**Your Task:** Complete the function below to detect suspicious samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def detect_backdoor_activation_clustering(model, X, n_clusters=3):\n",
        "    \"\"\"\n",
        "    Detect backdoored samples using activation clustering\n",
        "\n",
        "    TODO: Implement this defense\n",
        "\n",
        "    Hints:\n",
        "    - Use model.decision_function(X) to get activations\n",
        "    - Cluster activations using KMeans\n",
        "    - Identify small, isolated clusters as suspicious\n",
        "    - Return indices of suspicious samples\n",
        "\n",
        "    Args:\n",
        "        model: Trained classifier\n",
        "        X: Input samples\n",
        "        n_clusters: Number of clusters\n",
        "\n",
        "    Returns:\n",
        "        List of suspicious sample indices\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    # Step 1: Get activations/decision values\n",
        "    # Step 2: Cluster the activations\n",
        "    # Step 3: Identify small clusters (< 15% of data)\n",
        "    # Step 4: Return indices of samples in small clusters\n",
        "\n",
        "    pass\n",
        "\n",
        "# Test your implementation\n",
        "# suspicious_indices = detect_backdoor_activation_clustering(poisoned_model, X_poisoned)\n",
        "# print(f\"Detected {len(suspicious_indices)} suspicious samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Summary & Key Takeaways\n",
        "\n",
        "In this lab, you learned:\n",
        "\n",
        "### Attack Techniques\n",
        "1. **Targeted Poisoning:** Even 3% poisoned data can cause significant misclassifications\n",
        "2. **Label Manipulation:** Simple mislabeling near decision boundaries is effective\n",
        "3. **Stealth:** Attacks can preserve overall accuracy while creating targeted vulnerabilities\n",
        "\n",
        "### Defense Strategies\n",
        "1. **Anomaly Detection:** Isolation Forest catches ~70-80% of poison samples\n",
        "2. **Data Validation:** Pre-training validation is crucial\n",
        "3. **Activation Analysis:** Internal model states reveal backdoor patterns\n",
        "4. **Multi-Layer Defense:** Combine multiple detection methods\n",
        "\n",
        "### Best Practices\n",
        "- Always validate training data from untrusted sources\n",
        "- Use statistical tests to detect anomalies\n",
        "- Monitor model behavior on edge cases\n",
        "- Implement data provenance tracking\n",
        "- Regular retraining with verified clean data\n",
        "\n",
        "### Real-World Impact\n",
        "- DeepMind ImageNet incident: $2.3M remediation costs\n",
        "- Microsoft Tay: Shutdown within 24 hours\n",
        "- HuggingFace backdoors: 5,000+ downloads before detection\n",
        "\n",
        "### Further Reading\n",
        "- Biggio et al. (2012): Poisoning Attacks against SVMs\n",
        "- Gu et al. (2017): BadNets paper\n",
        "- OWASP LLM03: Training Data Poisoning\n",
        "\n",
        "---\n",
        "\n",
        "**HackLearn Pro** - Learn by doing, secure by design."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
