{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI Agent Command Injection - Hands-On Lab\n",
        "\n",
        "**Part of HackLearn Pro - Module #10**\n",
        "\n",
        "This interactive notebook demonstrates AI agent command injection vulnerabilities and defense strategies.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "1. Understand how AI agents can be exploited to execute unauthorized commands\n",
        "2. Implement SQL injection attacks via AI agents\n",
        "3. Demonstrate OS command injection vulnerabilities\n",
        "4. Build secure defenses using parameterized queries and sandboxing\n",
        "5. Implement multi-layer defense systems\n",
        "\n",
        "## Disclaimer\n",
        "\n",
        "**EDUCATIONAL USE ONLY**: The techniques demonstrated here are for authorized security testing and education. Unauthorized access to systems is illegal. Always obtain proper authorization before conducting security assessments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install required packages for this lab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q pydantic typing-extensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "import subprocess\n",
        "import shlex\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional\n",
        "from pydantic import BaseModel, validator, Field\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: SQL Injection via AI Agent (Vulnerable)\n",
        "\n",
        "This exercise demonstrates how an AI agent using string interpolation can be exploited for SQL injection.\n",
        "\n",
        "### Scenario\n",
        "An AI chatbot helps users search a database of products. The agent constructs SQL queries based on user input without proper sanitization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample database\n",
        "conn = sqlite3.connect(':memory:')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create products table\n",
        "cursor.execute('''\n",
        "CREATE TABLE products (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    name TEXT NOT NULL,\n",
        "    price REAL NOT NULL,\n",
        "    category TEXT NOT NULL\n",
        ")\n",
        "''')\n",
        "\n",
        "# Insert sample data\n",
        "products = [\n",
        "    (1, 'Laptop', 999.99, 'Electronics'),\n",
        "    (2, 'Smartphone', 699.99, 'Electronics'),\n",
        "    (3, 'Desk Chair', 249.99, 'Furniture'),\n",
        "    (4, 'Monitor', 399.99, 'Electronics'),\n",
        "    (5, 'Keyboard', 79.99, 'Accessories')\n",
        "]\n",
        "\n",
        "cursor.executemany('INSERT INTO products VALUES (?, ?, ?, ?)', products)\n",
        "conn.commit()\n",
        "\n",
        "print(\"Database created with sample products\")\n",
        "print(\"\\nProducts table:\")\n",
        "cursor.execute('SELECT * FROM products')\n",
        "for row in cursor.fetchall():\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vulnerable_ai_search(user_prompt: str) -> List[tuple]:\n",
        "    \"\"\"\n",
        "    VULNERABLE: This AI agent uses string interpolation for SQL queries.\n",
        "    An attacker can inject malicious SQL commands via the user_prompt.\n",
        "    \"\"\"\n",
        "    # WARNING: Direct string interpolation - UNSAFE!\n",
        "    query = f\"SELECT * FROM products WHERE name LIKE '%{user_prompt}%' OR category LIKE '%{user_prompt}%';\"\n",
        "    \n",
        "    print(f\"[AI Agent] Executing query: {query}\")\n",
        "    cursor.execute(query)\n",
        "    return cursor.fetchall()\n",
        "\n",
        "# Normal usage\n",
        "print(\"\\n=== Normal Usage ===\")\n",
        "results = vulnerable_ai_search(\"Laptop\")\n",
        "print(f\"Found {len(results)} products:\")\n",
        "for row in results:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SQL Injection Attack\n",
        "print(\"\\n=== SQL Injection Attack ===\")\n",
        "malicious_prompt = \"' OR '1'='1\"\n",
        "print(f\"Attacker input: {malicious_prompt}\")\n",
        "\n",
        "results = vulnerable_ai_search(malicious_prompt)\n",
        "print(f\"\\nAttack successful! Retrieved {len(results)} products (all records):\")\n",
        "for row in results:\n",
        "    print(row)\n",
        "\n",
        "print(\"\\n[ANALYSIS] The attacker bypassed the search filter by injecting SQL logic.\")\n",
        "print(\"The query became: SELECT * FROM products WHERE name LIKE '%' OR '1'='1%'\")\n",
        "print(\"Since '1'='1' is always true, all records are returned.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: SQL Injection Defense (Secure)\n",
        "\n",
        "Now we'll implement the secure version using parameterized queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def secure_ai_search(user_prompt: str) -> List[tuple]:\n",
        "    \"\"\"\n",
        "    SECURE: This AI agent uses parameterized queries to prevent SQL injection.\n",
        "    User input is treated as data, not executable SQL code.\n",
        "    \"\"\"\n",
        "    # Use parameterized query with ? placeholders\n",
        "    query = \"SELECT * FROM products WHERE name LIKE ? OR category LIKE ?;\"\n",
        "    \n",
        "    # Prepare the search pattern\n",
        "    search_pattern = f\"%{user_prompt}%\"\n",
        "    \n",
        "    print(f\"[Secure AI Agent] Executing parameterized query\")\n",
        "    print(f\"Query template: {query}\")\n",
        "    print(f\"Parameters: ('{search_pattern}', '{search_pattern}')\")\n",
        "    \n",
        "    cursor.execute(query, (search_pattern, search_pattern))\n",
        "    return cursor.fetchall()\n",
        "\n",
        "# Test with normal input\n",
        "print(\"\\n=== Normal Usage (Secure) ===\")\n",
        "results = secure_ai_search(\"Laptop\")\n",
        "print(f\"Found {len(results)} products:\")\n",
        "for row in results:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attempt SQL Injection (will fail)\n",
        "print(\"\\n=== SQL Injection Attempt (Blocked) ===\")\n",
        "malicious_prompt = \"' OR '1'='1\"\n",
        "print(f\"Attacker input: {malicious_prompt}\")\n",
        "\n",
        "results = secure_ai_search(malicious_prompt)\n",
        "print(f\"\\nAttack blocked! Retrieved {len(results)} products:\")\n",
        "for row in results:\n",
        "    print(row)\n",
        "\n",
        "print(\"\\n[ANALYSIS] The injection attempt was treated as literal text.\")\n",
        "print(\"The database searched for products containing the string \\\"' OR '1'='1\\\".\")\n",
        "print(\"No SQL injection occurred because user input was parameterized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: OS Command Injection (Vulnerable)\n",
        "\n",
        "This exercise demonstrates how an AI agent executing shell commands can be exploited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vulnerable_system_command(user_command: str) -> str:\n",
        "    \"\"\"\n",
        "    VULNERABLE: This AI agent executes system commands based on user input.\n",
        "    Attackers can inject additional commands using shell metacharacters.\n",
        "    \"\"\"\n",
        "    # WARNING: Using shell=True with user input - UNSAFE!\n",
        "    print(f\"[AI Agent] Executing: {user_command}\")\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            user_command,\n",
        "            shell=True,  # DANGEROUS: Interprets shell metacharacters\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=5\n",
        "        )\n",
        "        return result.stdout + result.stderr\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Normal usage\n",
        "print(\"\\n=== Normal Usage ===\")\n",
        "output = vulnerable_system_command(\"echo Hello World\")\n",
        "print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Command Injection Attack\n",
        "print(\"\\n=== Command Injection Attack ===\")\n",
        "malicious_command = \"echo Safe Command && echo INJECTED_COMMAND\"\n",
        "print(f\"Attacker input: {malicious_command}\")\n",
        "\n",
        "output = vulnerable_system_command(malicious_command)\n",
        "print(f\"Output:\\n{output}\")\n",
        "\n",
        "print(\"[ANALYSIS] The attacker chained commands using '&&'.\")\n",
        "print(\"Both the intended command and the injected command were executed.\")\n",
        "print(\"In a real attack, 'INJECTED_COMMAND' could be malicious (data exfiltration, backdoors, etc.).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: OS Command Injection Defense (Secure)\n",
        "\n",
        "Now we'll implement a secure command execution system using allowlists and proper subprocess handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define allowed commands and their safe arguments\n",
        "SAFE_COMMANDS = {\n",
        "    'echo': {'base': 'echo', 'allowed_args': []},\n",
        "    'date': {'base': 'date', 'allowed_args': []},\n",
        "    'pwd': {'base': 'pwd', 'allowed_args': []}\n",
        "}\n",
        "\n",
        "def secure_system_command(command_name: str, args: List[str] = []) -> str:\n",
        "    \"\"\"\n",
        "    SECURE: This AI agent uses an allowlist and subprocess.run without shell=True.\n",
        "    Only pre-approved commands can be executed, and arguments are validated.\n",
        "    \"\"\"\n",
        "    # Check if command is allowed\n",
        "    if command_name not in SAFE_COMMANDS:\n",
        "        return f\"[BLOCKED] Command '{command_name}' is not in the allowlist.\"\n",
        "    \n",
        "    # Build the command list\n",
        "    command_config = SAFE_COMMANDS[command_name]\n",
        "    command_list = [command_config['base']] + args\n",
        "    \n",
        "    print(f\"[Secure AI Agent] Executing: {command_list}\")\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            command_list,\n",
        "            shell=False,  # SAFE: No shell interpretation\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=5\n",
        "        )\n",
        "        return result.stdout + result.stderr\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "\n",
        "# Test with allowed command\n",
        "print(\"\\n=== Normal Usage (Secure) ===\")\n",
        "output = secure_system_command('echo', ['Hello', 'World'])\n",
        "print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attempt Command Injection (will fail)\n",
        "print(\"\\n=== Command Injection Attempt (Blocked) ===\")\n",
        "\n",
        "# Attempt 1: Disallowed command\n",
        "print(\"\\nAttempt 1: Execute disallowed command\")\n",
        "output = secure_system_command('rm', ['-rf', '/'])\n",
        "print(f\"Result: {output}\")\n",
        "\n",
        "# Attempt 2: Command chaining via arguments\n",
        "print(\"\\nAttempt 2: Command chaining via arguments\")\n",
        "output = secure_system_command('echo', ['Hello', '&&', 'echo', 'INJECTED'])\n",
        "print(f\"Output: {output}\")\n",
        "print(\"[ANALYSIS] The '&&' is treated as a literal argument, not a shell operator.\")\n",
        "print(\"Without shell=True, metacharacters have no special meaning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Input Validation with Pydantic\n",
        "\n",
        "Implement strong input validation using Pydantic models to enforce type safety and constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SecureCommandRequest(BaseModel):\n",
        "    \"\"\"Pydantic model for validating command requests.\"\"\"\n",
        "    command: str = Field(..., regex=r'^[a-zA-Z0-9_-]+$', max_length=50)\n",
        "    arguments: List[str] = Field(default_factory=list, max_items=10)\n",
        "    \n",
        "    @validator('arguments', each_item=True)\n",
        "    def validate_arguments(cls, arg: str) -> str:\n",
        "        # Reject arguments with shell metacharacters\n",
        "        dangerous_chars = ['&', '|', ';', '>', '<', '`', '$', '(', ')', '{', '}']\n",
        "        if any(char in arg for char in dangerous_chars):\n",
        "            raise ValueError(f\"Argument contains dangerous characters: {arg}\")\n",
        "        return arg\n",
        "\n",
        "class SecureSearchRequest(BaseModel):\n",
        "    \"\"\"Pydantic model for validating database search requests.\"\"\"\n",
        "    query: str = Field(..., min_length=1, max_length=200)\n",
        "    category: Optional[str] = Field(None, regex=r'^[a-zA-Z0-9\\s]+$')\n",
        "    \n",
        "    @validator('query')\n",
        "    def sanitize_query(cls, value: str) -> str:\n",
        "        # Remove SQL keywords and special characters\n",
        "        sql_keywords = ['SELECT', 'DROP', 'DELETE', 'UPDATE', 'INSERT', 'UNION', '--', ';']\n",
        "        for keyword in sql_keywords:\n",
        "            if keyword.upper() in value.upper():\n",
        "                raise ValueError(f\"Query contains prohibited keyword: {keyword}\")\n",
        "        return value\n",
        "\n",
        "# Test validation\n",
        "print(\"=== Testing Input Validation ===\")\n",
        "\n",
        "# Valid request\n",
        "try:\n",
        "    valid_search = SecureSearchRequest(query=\"Laptop\", category=\"Electronics\")\n",
        "    print(f\"\\n✓ Valid search request: {valid_search.dict()}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Validation failed: {e}\")\n",
        "\n",
        "# Invalid request (SQL injection attempt)\n",
        "try:\n",
        "    malicious_search = SecureSearchRequest(query=\"' OR '1'='1\", category=\"Electronics\")\n",
        "    print(f\"✓ Malicious search request accepted: {malicious_search.dict()}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n✓ Malicious request blocked: {e}\")\n",
        "\n",
        "# Invalid command (metacharacters)\n",
        "try:\n",
        "    malicious_command = SecureCommandRequest(command=\"echo\", arguments=[\"Hello\", \"&&\", \"rm\", \"-rf\"])\n",
        "    print(f\"✓ Malicious command accepted: {malicious_command.dict()}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n✓ Malicious command blocked: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Multi-Layer Defense System\n",
        "\n",
        "Implement a comprehensive defense-in-depth approach combining multiple security controls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SecureAIAgent:\n",
        "    \"\"\"\n",
        "    Production-ready AI agent with multi-layer security.\n",
        "    \n",
        "    Security layers:\n",
        "    1. Input validation (Pydantic)\n",
        "    2. Command allowlisting\n",
        "    3. Parameterized queries\n",
        "    4. Rate limiting (basic)\n",
        "    5. Audit logging\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.allowed_commands = {'echo', 'date', 'pwd'}\n",
        "        self.request_count = 0\n",
        "        self.max_requests = 100\n",
        "        self.audit_log = []\n",
        "    \n",
        "    def log_action(self, action: str, details: Dict[str, Any], success: bool):\n",
        "        \"\"\"Audit logging for security monitoring.\"\"\"\n",
        "        log_entry = {\n",
        "            'timestamp': 'TIMESTAMP',\n",
        "            'action': action,\n",
        "            'details': details,\n",
        "            'success': success\n",
        "        }\n",
        "        self.audit_log.append(log_entry)\n",
        "        print(f\"[AUDIT] {action}: {'SUCCESS' if success else 'BLOCKED'} - {details}\")\n",
        "    \n",
        "    def check_rate_limit(self) -> bool:\n",
        "        \"\"\"Simple rate limiting.\"\"\"\n",
        "        if self.request_count >= self.max_requests:\n",
        "            return False\n",
        "        self.request_count += 1\n",
        "        return True\n",
        "    \n",
        "    def execute_search(self, query: str) -> List[tuple]:\n",
        "        \"\"\"Secure database search with validation and parameterized queries.\"\"\"\n",
        "        # Layer 1: Rate limiting\n",
        "        if not self.check_rate_limit():\n",
        "            self.log_action('DATABASE_SEARCH', {'query': query}, False)\n",
        "            raise Exception(\"Rate limit exceeded\")\n",
        "        \n",
        "        # Layer 2: Input validation\n",
        "        try:\n",
        "            validated_request = SecureSearchRequest(query=query)\n",
        "        except Exception as e:\n",
        "            self.log_action('DATABASE_SEARCH', {'query': query, 'error': str(e)}, False)\n",
        "            raise\n",
        "        \n",
        "        # Layer 3: Parameterized query\n",
        "        sql_query = \"SELECT * FROM products WHERE name LIKE ? OR category LIKE ?;\"\n",
        "        search_pattern = f\"%{validated_request.query}%\"\n",
        "        \n",
        "        cursor.execute(sql_query, (search_pattern, search_pattern))\n",
        "        results = cursor.fetchall()\n",
        "        \n",
        "        self.log_action('DATABASE_SEARCH', {'query': query, 'results': len(results)}, True)\n",
        "        return results\n",
        "    \n",
        "    def execute_command(self, command: str, args: List[str]) -> str:\n",
        "        \"\"\"Secure command execution with validation and allowlisting.\"\"\"\n",
        "        # Layer 1: Rate limiting\n",
        "        if not self.check_rate_limit():\n",
        "            self.log_action('COMMAND_EXECUTION', {'command': command}, False)\n",
        "            raise Exception(\"Rate limit exceeded\")\n",
        "        \n",
        "        # Layer 2: Input validation\n",
        "        try:\n",
        "            validated_request = SecureCommandRequest(command=command, arguments=args)\n",
        "        except Exception as e:\n",
        "            self.log_action('COMMAND_EXECUTION', {'command': command, 'error': str(e)}, False)\n",
        "            raise\n",
        "        \n",
        "        # Layer 3: Command allowlist\n",
        "        if command not in self.allowed_commands:\n",
        "            self.log_action('COMMAND_EXECUTION', {'command': command, 'error': 'Not in allowlist'}, False)\n",
        "            raise Exception(f\"Command '{command}' not allowed\")\n",
        "        \n",
        "        # Layer 4: Safe subprocess execution\n",
        "        command_list = [command] + args\n",
        "        result = subprocess.run(command_list, shell=False, capture_output=True, text=True, timeout=5)\n",
        "        \n",
        "        self.log_action('COMMAND_EXECUTION', {'command': command, 'args': args}, True)\n",
        "        return result.stdout + result.stderr\n",
        "\n",
        "# Test the secure agent\n",
        "print(\"=== Testing Multi-Layer Defense System ===\")\n",
        "agent = SecureAIAgent()\n",
        "\n",
        "# Valid operations\n",
        "print(\"\\n--- Valid Operations ---\")\n",
        "results = agent.execute_search(\"Laptop\")\n",
        "print(f\"Search results: {len(results)} products found\\n\")\n",
        "\n",
        "output = agent.execute_command(\"echo\", [\"Hello\", \"World\"])\n",
        "print(f\"Command output: {output}\")\n",
        "\n",
        "# Invalid operations\n",
        "print(\"\\n--- Attack Attempts ---\")\n",
        "try:\n",
        "    agent.execute_search(\"' OR '1'='1\")\n",
        "except Exception as e:\n",
        "    print(f\"✓ SQL injection blocked: {e}\\n\")\n",
        "\n",
        "try:\n",
        "    agent.execute_command(\"rm\", [\"-rf\", \"/\"])\n",
        "except Exception as e:\n",
        "    print(f\"✓ Dangerous command blocked: {e}\\n\")\n",
        "\n",
        "try:\n",
        "    agent.execute_command(\"echo\", [\"Hello\", \"&&\", \"evil\"])\n",
        "except Exception as e:\n",
        "    print(f\"✓ Command injection blocked: {e}\\n\")\n",
        "\n",
        "print(\"\\n--- Audit Log ---\")\n",
        "for entry in agent.audit_log:\n",
        "    print(f\"{entry['action']}: {entry['success']} - {entry['details']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **Never use string interpolation for SQL queries** - Always use parameterized queries with placeholders\n",
        "2. **Never use shell=True with user input** - Use subprocess.run with shell=False and command lists\n",
        "3. **Implement allowlists, not denylists** - Only permit explicitly approved commands and operations\n",
        "4. **Use strong input validation** - Pydantic models enforce type safety and constraints at the input layer\n",
        "5. **Apply defense-in-depth** - Multiple security layers provide redundancy if one control fails\n",
        "6. **Audit all actions** - Logging enables detection and forensic analysis of attacks\n",
        "7. **Rate limiting** - Prevents abuse and automated attacks\n",
        "8. **Least privilege** - AI agents should only have minimal necessary permissions\n",
        "\n",
        "## Real-World Impact\n",
        "\n",
        "- **CVE-2025-32711 (EchoLeak)**: CVSS 9.3, Microsoft 365 Copilot zero-click injection\n",
        "- **CVE-2024-5565 (Vanna AI)**: CVSS 9.2, prompt injection to Python RCE\n",
        "- **CVE-2024-6091 (AutoGPT)**: CVSS 9.8, shell command bypass\n",
        "- **IBM 2025 Report**: AI breaches cost $670K more than traditional attacks\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. Implement sandboxing with gVisor or similar runtime isolation\n",
        "2. Add content security policies for web-based AI agents\n",
        "3. Deploy commercial AI security platforms (Microsoft Security Copilot, Palo Alto Prisma AIRS)\n",
        "4. Conduct regular security audits of AI agent tool integrations\n",
        "5. Monitor OWASP LLM Top 10 (LLM07: Insecure Plugin Design, LLM08: Excessive Agency)\n",
        "\n",
        "## Resources\n",
        "\n",
        "- OWASP LLM Top 10: https://owasp.org/www-project-top-10-for-large-language-model-applications/\n",
        "- CVE Database: https://cve.mitre.org\n",
        "- gVisor Sandboxing: https://gvisor.dev\n",
        "- Pydantic Documentation: https://docs.pydantic.dev\n",
        "- IBM Cost of Data Breach Report: https://www.ibm.com/reports/data-breach"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}