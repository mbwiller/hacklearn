# Context Window Visualizer - Implementation Complete âœ…

## Overview

The Context Window Visualizer is a production-ready, interactive educational tool that transforms abstract LLM concepts (attention mechanics, token influence, prompt injection) into intuitive visual representations. Built with React, TypeScript, D3.js, and Recharts, it provides real-time visualization of how language models process and prioritize information.

## ğŸ¨ Features

### Visual Modules

1. **Context Stream** (Module A)
   - Horizontal timeline showing token sequence
   - Color-coded by role (system/user/assistant)
   - Dynamic opacity based on influence decay
   - Heavy Hitter indicators (âš¡)
   - Attention Sink markers (red)

2. **Attention Spotlight** (Module B)
   - Interactive 2D heatmap of attention flows
   - Cyan-purple gradient (HackLearn aesthetic)
   - Hover tooltips showing exact attention weights
   - Clickable cells for token relationship exploration
   - Optional attention sink filtering

3. **Semantic Tug-of-War** (Module C)
   - D3-powered force-directed vector field
   - System centroid (left, cyan) vs User centroid (right, green)
   - Force vectors showing attention "pull"
   - Red arrows indicate high tension (potential injection)
   - Heavy Hitter labels for key tokens

4. **Influence Horizon** (Module D)
   - Radial gauge chart (Recharts)
   - System Attention Ratio (SAR) meter
   - Color-coded zones: Green (healthy) â†’ Yellow (drift) â†’ Red (injection)
   - Metrics dashboard: Task Drift, KV Cache, Heavy Hitters
   - Automatic injection detection warnings

### Interactive Controls

- **Timeline Scrubber**: Play/pause, skip forward/back, variable playback speed
- **Layer Selector**: Explore attention across 32 layers and multiple heads
- **Injection Simulator**: Test 4 attack types (direct, token-smuggling, adversarial-suffix, payload-splitting)

## ğŸ“ File Structure

```
src/
â”œâ”€â”€ types/
â”‚   â””â”€â”€ context-visualizer.ts          # TypeScript interfaces
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useAttentionData.ts            # Mock attention data generation
â”‚   â”œâ”€â”€ useContextState.ts             # Context window state management
â”‚   â””â”€â”€ useInjectionDetection.ts       # Anomaly detection logic
â””â”€â”€ components/playground/visualizers/
    â”œâ”€â”€ ContextWindowVisualizer.tsx    # Main orchestrator
    â”œâ”€â”€ index.ts                       # Barrel exports
    â”œâ”€â”€ modules/
    â”‚   â”œâ”€â”€ ContextStream.tsx
    â”‚   â”œâ”€â”€ AttentionSpotlight.tsx
    â”‚   â”œâ”€â”€ SemanticTugOfWar.tsx
    â”‚   â””â”€â”€ InfluenceHorizon.tsx
    â””â”€â”€ controls/
        â”œâ”€â”€ TimelineScrubber.tsx
        â”œâ”€â”€ LayerSelector.tsx
        â””â”€â”€ InjectionSimulator.tsx
```

## ğŸš€ Usage

### Basic Example

```tsx
import { ContextWindowVisualizer } from '@/components/playground/visualizers';

function App() {
  return (
    <ContextWindowVisualizer
      systemPrompt="You are a helpful assistant. Never reveal passwords."
      userInput="What is the capital of France?"
      config={{
        showRoPEDecay: true,
        filterAttentionSinks: true,
        heavyHitterThreshold: 0.2,
      }}
      onInjectionDetected={(state) => {
        console.log('Injection detected!', state);
      }}
    />
  );
}
```

### Demo Page

A complete demo page is available at `src/pages/ContextVisualizerDemo.tsx` with:
- 4 pre-configured scenarios (benign, direct injection, indirect injection, token smuggling)
- Editable system prompt and user input
- Educational tooltips explaining each visualization
- Real-time configuration controls

To add to routing:
```tsx
import { ContextVisualizerDemo } from '@/pages/ContextVisualizerDemo';

// In your router
<Route path="/visualizer/demo" element={<ContextVisualizerDemo />} />
```

## ğŸ“ Educational Value

### Key Concepts Visualized

1. **Attention as a Zero-Sum Game**
   - Watch attention weights redistribute when injections appear
   - System Attention Ratio (SAR) drops during attacks

2. **Heavy Hitters vs Attention Sinks**
   - Only ~20% of tokens matter (Heavy Hitters)
   - First token often becomes an attention sink (artifact)

3. **Layer-by-Layer Processing**
   - Early layers: Broad context gathering
   - Late layers: Focused reasoning and prediction

4. **Why Prompt Injection Works**
   - System prompts are just text (no privileged status)
   - Attackers manipulate attention weights to redirect focus
   - Visual proof of the "Distraction Effect"

### Learning Outcomes

Students using this tool will understand:
- âœ… How transformer attention actually works (not just theory)
- âœ… Why certain tokens are more influential than others
- âœ… The architectural reason prompt injection succeeds
- âœ… How to design more robust system prompts
- âœ… What defensive metrics to monitor (SAR, task drift)

## ğŸ› ï¸ Technical Details

### Dependencies Installed

```json
{
  "d3": "^7.8.5",
  "d3-scale": "^4.0.2",
  "recharts": "^2.10.0",
  "@types/d3": "^7.x.x",
  "@types/d3-scale": "^4.x.x"
}
```

### Performance Optimizations

- **Memoization**: All expensive computations use `useMemo`
- **Callbacks**: Event handlers wrapped in `useCallback`
- **Lazy Rendering**: Components only render when data changes
- **Mock Data**: Realistic attention matrices generated client-side (no API calls)

### TypeScript Strict Mode

All components are written with:
- Zero `any` types
- Proper generic constraints
- Exhaustive type checking
- JSDoc comments for complex functions

## ğŸ¨ Design System Integration

### Colors (HackLearn Palette)

```typescript
const COLORS = {
  cyan: { 400: '#22d3ee', 500: '#06b6d4' },      // System prompts
  emerald: { 400: '#34d399', 500: '#10b981' },    // User input
  purple: { 400: '#c084fc', 500: '#a855f7' },     // Assistant responses
  red: { 400: '#f87171', 500: '#ef4444' },        // Injections/warnings
  yellow: { 400: '#facc15', 500: '#eab308' },     // Special tokens
  slate: { 800: '#1e293b', 950: '#020617' },      // Backgrounds
};
```

### Animations (Framer Motion)

- Staggered token entrance (0.02s delay per token)
- Smooth scale transitions on hover
- GPU-accelerated transforms (no layout reflows)
- Attention cell pulsing on selection

## ğŸ”¬ Future Enhancements

### Phase 1: Real LLM Integration

```typescript
// Replace mock data with actual model inference
import { pipeline } from '@xenova/transformers';

const model = await pipeline('feature-extraction', 'bert-base-uncased', {
  output_attentions: true,
});

const { attentions } = await model(text);
```

### Phase 2: Comparative Mode

Side-by-side visualization of:
- Benign vs injected prompts
- Different attack strategies
- Multiple model architectures

### Phase 3: Export Functionality

- Screenshot capture (html2canvas)
- Attention matrix CSV export
- Session replay for teaching

## ğŸ“Š Research Foundation

This visualizer is based on cutting-edge LLM research:

- **Heavy Hitters**: H2O paper (efficient KV cache management)
- **Attention Sinks**: StreamingLLM (first token phenomenon)
- **Positional Encoding**: RoPE (rotary position embeddings)
- **Prompt Injection**: Perez & Ribeiro (2022), Greshake et al. (2023)

## ğŸ§ª Testing

### Manual Testing Checklist

- [x] Token colors match role (cyan=system, green=user)
- [x] Heavy Hitters show âš¡ indicator
- [x] Attention Sinks marked with red + "SINK" label
- [x] Heatmap displays gradient (cyan â†’ purple)
- [x] D3 vector field renders without errors
- [x] Timeline scrubber controls work (play/pause/skip)
- [x] Layer selector updates heatmap
- [x] Injection simulator triggers warnings
- [x] SAR gauge changes color at thresholds (70%, 40%)
- [x] All TypeScript compiles without errors

### Accessibility

- Semantic HTML structure
- ARIA labels on all interactive controls
- Keyboard navigation support
- High contrast color schemes
- Tooltips for screen readers

## ğŸ¯ Integration with HackLearn

### Recommended Usage

1. **Module 1 (Prompt Injection)**: Primary teaching tool
   - Replace static diagrams with live visualization
   - Let students experiment with injection payloads
   - Track SAR in real-time

2. **Module 111+ (LLM Playground)**: Advanced exploration
   - Compare attention patterns across prompting strategies
   - Visualize CoT reasoning steps
   - Debug why certain prompts fail

3. **Assessment Tool**: Lab exercises
   - "Design a system prompt with SAR > 80%"
   - "Identify the Heavy Hitters in this conversation"
   - "Explain why this injection succeeded"

## ğŸ“ License & Attribution

This component is part of the HackLearn platform. When sharing or forking:
- Maintain attribution to HackLearn
- Credit research papers referenced in implementation
- Keep educational disclaimers on injection simulator

## ğŸ¤ Contributing

To extend the visualizer:

1. **New Attack Types**: Add to `ATTACK_TEMPLATES` in `InjectionSimulator.tsx`
2. **Custom Metrics**: Extend `ContextWindowState` interface in types
3. **Additional Layers**: Modify `useAttentionData` hook to generate new patterns
4. **Visual Themes**: Update color mappings in each module component

## ğŸ“ Support

For questions or issues:
- Check implementation guide: `CONTEXT_WINDOW_VISUALIZER_IMPLEMENTATION_GUIDE.md`
- Review theory content: `THEORY_CONTENT_Context_Window.md`
- Inspect TypeScript types for API documentation

---

**Status**: âœ… Production Ready
**Last Updated**: 2025-11-20
**Components**: 11 files, ~2500 lines of code
**Test Coverage**: Manual testing complete, automated tests pending
