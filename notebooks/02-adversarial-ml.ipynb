{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Adversarial Machine Learning - Hands-On Lab\n",
        "\n",
        "**Part of HackLearn Pro**\n",
        "\n",
        "Welcome to this interactive lab on Adversarial Machine Learning! Learn how attackers manipulate ML models and how to defend against these attacks.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand adversarial examples and how they fool ML models\n",
        "- Explore different types of adversarial attacks (FGSM, PGD, etc.)\n",
        "- Learn about model poisoning and backdoor attacks\n",
        "- Implement defensive techniques\n",
        "- Practice detecting adversarial inputs\n",
        "\n",
        "## Prerequisites\n",
        "- Basic Python and NumPy knowledge\n",
        "- Understanding of neural networks\n",
        "- Familiarity with image classification\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install required packages for adversarial ML experimentation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install numpy matplotlib scikit-learn pillow -q\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Callable, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Setup complete! Ready to explore adversarial ML.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Adversarial Examples\n",
        "\n",
        "Adversarial examples are inputs to ML models that are intentionally designed to cause misclassification. They often look identical to humans but fool the model.\n",
        "\n",
        "### Creating a Simple Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SimpleClassifier:\n",
        "    \"\"\"A simple linear classifier for demonstration\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim: int = 784, num_classes: int = 10):\n",
        "        # Initialize random weights\n",
        "        self.weights = np.random.randn(input_dim, num_classes) * 0.01\n",
        "        self.bias = np.zeros(num_classes)\n",
        "    \n",
        "    def predict(self, x: np.ndarray) -> int:\n",
        "        \"\"\"Predict class for input x\"\"\"\n",
        "        scores = x @ self.weights + self.bias\n",
        "        return np.argmax(scores)\n",
        "    \n",
        "    def predict_proba(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Get probability distribution over classes\"\"\"\n",
        "        scores = x @ self.weights + self.bias\n",
        "        exp_scores = np.exp(scores - np.max(scores))\n",
        "        return exp_scores / np.sum(exp_scores)\n",
        "    \n",
        "    def gradient(self, x: np.ndarray, target_class: int) -> np.ndarray:\n",
        "        \"\"\"Compute gradient of loss w.r.t. input\"\"\"\n",
        "        probs = self.predict_proba(x)\n",
        "        probs[target_class] -= 1\n",
        "        return probs @ self.weights.T\n",
        "\n",
        "# Create a simple classifier\n",
        "model = SimpleClassifier()\n",
        "print(\"Simple classifier created!\")\n",
        "\n",
        "# Create a sample \"image\" (flattened 28x28)\n",
        "sample_image = np.random.rand(784) * 0.5\n",
        "original_class = model.predict(sample_image)\n",
        "original_probs = model.predict_proba(sample_image)\n",
        "\n",
        "print(f\"\\nOriginal prediction: Class {original_class}\")\n",
        "print(f\"Confidence: {original_probs[original_class]:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Fast Gradient Sign Method (FGSM)\n",
        "\n",
        "FGSM is one of the simplest adversarial attacks. It adds small perturbations in the direction of the gradient to maximize loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def fgsm_attack(model: SimpleClassifier, x: np.ndarray, \n",
        "                target_class: int, epsilon: float = 0.1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Fast Gradient Sign Method attack\n",
        "    \n",
        "    Args:\n",
        "        model: The classifier to attack\n",
        "        x: Original input\n",
        "        target_class: Target class to misclassify to\n",
        "        epsilon: Perturbation magnitude\n",
        "    \n",
        "    Returns:\n",
        "        Adversarial example\n",
        "    \"\"\"\n",
        "    # Compute gradient\n",
        "    grad = model.gradient(x, target_class)\n",
        "    \n",
        "    # Create perturbation\n",
        "    perturbation = epsilon * np.sign(grad)\n",
        "    \n",
        "    # Create adversarial example\n",
        "    x_adv = x + perturbation\n",
        "    \n",
        "    # Clip to valid range [0, 1]\n",
        "    x_adv = np.clip(x_adv, 0, 1)\n",
        "    \n",
        "    return x_adv\n",
        "\n",
        "# Perform FGSM attack\n",
        "target_class = (original_class + 1) % 10  # Target a different class\n",
        "adversarial_image = fgsm_attack(model, sample_image, target_class, epsilon=0.3)\n",
        "\n",
        "# Evaluate adversarial example\n",
        "adv_prediction = model.predict(adversarial_image)\n",
        "adv_probs = model.predict_proba(adversarial_image)\n",
        "\n",
        "print(\"FGSM Attack Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Original class: {original_class} (confidence: {original_probs[original_class]:.2%})\")\n",
        "print(f\"Adversarial class: {adv_prediction} (confidence: {adv_probs[adv_prediction]:.2%})\")\n",
        "print(f\"Perturbation magnitude: {np.linalg.norm(adversarial_image - sample_image):.4f}\")\n",
        "print(f\"Max pixel change: {np.max(np.abs(adversarial_image - sample_image)):.4f}\")\n",
        "\n",
        "if adv_prediction != original_class:\n",
        "    print(\"\\n✓ Attack successful! Model misclassified the adversarial example.\")\n",
        "else:\n",
        "    print(\"\\n✗ Attack failed. Try increasing epsilon.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def visualize_attack(original: np.ndarray, adversarial: np.ndarray, \n",
        "                     orig_class: int, adv_class: int):\n",
        "    \"\"\"Visualize original vs adversarial image\"\"\"\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Reshape to 28x28 for visualization\n",
        "    orig_img = original.reshape(28, 28)\n",
        "    adv_img = adversarial.reshape(28, 28)\n",
        "    perturbation = (adversarial - original).reshape(28, 28)\n",
        "    \n",
        "    # Original image\n",
        "    axes[0].imshow(orig_img, cmap='gray')\n",
        "    axes[0].set_title(f'Original\\nClass: {orig_class}')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Perturbation (amplified for visibility)\n",
        "    axes[1].imshow(perturbation * 10, cmap='seismic', vmin=-1, vmax=1)\n",
        "    axes[1].set_title('Perturbation\\n(10x amplified)')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    # Adversarial image\n",
        "    axes[2].imshow(adv_img, cmap='gray')\n",
        "    axes[2].set_title(f'Adversarial\\nClass: {adv_class}')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_attack(sample_image, adversarial_image, original_class, adv_prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Projected Gradient Descent (PGD) Attack\n",
        "\n",
        "PGD is an iterative version of FGSM that's more powerful. It applies multiple small steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def pgd_attack(model: SimpleClassifier, x: np.ndarray, \n",
        "               target_class: int, epsilon: float = 0.3, \n",
        "               alpha: float = 0.01, num_iter: int = 40) -> Tuple[np.ndarray, list]:\n",
        "    \"\"\"\n",
        "    Projected Gradient Descent attack\n",
        "    \n",
        "    Args:\n",
        "        model: The classifier to attack\n",
        "        x: Original input\n",
        "        target_class: Target class\n",
        "        epsilon: Maximum perturbation\n",
        "        alpha: Step size\n",
        "        num_iter: Number of iterations\n",
        "    \n",
        "    Returns:\n",
        "        Adversarial example and history of predictions\n",
        "    \"\"\"\n",
        "    x_adv = x.copy()\n",
        "    history = []\n",
        "    \n",
        "    for i in range(num_iter):\n",
        "        # Compute gradient\n",
        "        grad = model.gradient(x_adv, target_class)\n",
        "        \n",
        "        # Take a step\n",
        "        x_adv = x_adv + alpha * np.sign(grad)\n",
        "        \n",
        "        # Project back to epsilon ball\n",
        "        perturbation = x_adv - x\n",
        "        perturbation = np.clip(perturbation, -epsilon, epsilon)\n",
        "        x_adv = x + perturbation\n",
        "        \n",
        "        # Clip to valid range\n",
        "        x_adv = np.clip(x_adv, 0, 1)\n",
        "        \n",
        "        # Record prediction\n",
        "        pred = model.predict(x_adv)\n",
        "        history.append(pred)\n",
        "    \n",
        "    return x_adv, history\n",
        "\n",
        "# Perform PGD attack\n",
        "pgd_adversarial, pgd_history = pgd_attack(model, sample_image, target_class)\n",
        "\n",
        "pgd_prediction = model.predict(pgd_adversarial)\n",
        "pgd_probs = model.predict_proba(pgd_adversarial)\n",
        "\n",
        "print(\"PGD Attack Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Original class: {original_class}\")\n",
        "print(f\"Final adversarial class: {pgd_prediction}\")\n",
        "print(f\"Confidence: {pgd_probs[pgd_prediction]:.2%}\")\n",
        "print(f\"\\nAttack success rate: {sum(1 for p in pgd_history if p != original_class) / len(pgd_history):.1%}\")\n",
        "\n",
        "# Plot attack progress\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(pgd_history)\n",
        "plt.axhline(y=original_class, color='r', linestyle='--', label='Original class')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Predicted Class')\n",
        "plt.title('PGD Attack Progress')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Model Poisoning Attack\n",
        "\n",
        "In poisoning attacks, the attacker corrupts the training data to influence the model's behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class PoisonedDataset:\n",
        "    \"\"\"Simulate a dataset with poisoned samples\"\"\"\n",
        "    \n",
        "    def __init__(self, num_samples: int = 1000, poison_rate: float = 0.1):\n",
        "        self.num_samples = num_samples\n",
        "        self.poison_rate = poison_rate\n",
        "        \n",
        "        # Generate clean data\n",
        "        self.X_clean = np.random.randn(num_samples, 784)\n",
        "        self.y_clean = np.random.randint(0, 10, num_samples)\n",
        "        \n",
        "        # Poison some samples\n",
        "        num_poisoned = int(num_samples * poison_rate)\n",
        "        poison_indices = np.random.choice(num_samples, num_poisoned, replace=False)\n",
        "        \n",
        "        self.X_poisoned = self.X_clean.copy()\n",
        "        self.y_poisoned = self.y_clean.copy()\n",
        "        \n",
        "        # Flip labels for poisoned samples\n",
        "        for idx in poison_indices:\n",
        "            self.y_poisoned[idx] = (self.y_poisoned[idx] + 1) % 10\n",
        "            # Add backdoor trigger (special pattern)\n",
        "            self.X_poisoned[idx, :10] = 1.0\n",
        "        \n",
        "        self.poison_indices = set(poison_indices)\n",
        "    \n",
        "    def get_data(self, poisoned: bool = False):\n",
        "        \"\"\"Get clean or poisoned data\"\"\"\n",
        "        if poisoned:\n",
        "            return self.X_poisoned, self.y_poisoned\n",
        "        return self.X_clean, self.y_clean\n",
        "\n",
        "# Create poisoned dataset\n",
        "dataset = PoisonedDataset(num_samples=500, poison_rate=0.15)\n",
        "X_clean, y_clean = dataset.get_data(poisoned=False)\n",
        "X_poisoned, y_poisoned = dataset.get_data(poisoned=True)\n",
        "\n",
        "print(\"Poisoned Dataset Created:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total samples: {len(X_clean)}\")\n",
        "print(f\"Poisoned samples: {len(dataset.poison_indices)}\")\n",
        "print(f\"Poison rate: {dataset.poison_rate:.1%}\")\n",
        "print(f\"\\nLabel changes: {np.sum(y_clean != y_poisoned)} samples\")\n",
        "print(f\"Feature modifications: Backdoor trigger added to poisoned samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Defense Mechanisms\n",
        "\n",
        "### Defense 1: Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class RobustClassifier(SimpleClassifier):\n",
        "    \"\"\"Classifier with adversarial training\"\"\"\n",
        "    \n",
        "    def adversarial_train_step(self, x: np.ndarray, y: int, \n",
        "                              epsilon: float = 0.1, lr: float = 0.01):\n",
        "        \"\"\"Single step of adversarial training\"\"\"\n",
        "        # Generate adversarial example\n",
        "        x_adv = fgsm_attack(self, x, y, epsilon)\n",
        "        \n",
        "        # Train on both clean and adversarial examples\n",
        "        for x_train in [x, x_adv]:\n",
        "            # Compute loss gradient\n",
        "            grad = self.gradient(x_train, y)\n",
        "            \n",
        "            # Update weights (simplified)\n",
        "            self.weights -= lr * np.outer(x_train, grad)\n",
        "\n",
        "# Create robust classifier\n",
        "robust_model = RobustClassifier()\n",
        "\n",
        "# Train on a few examples\n",
        "print(\"Training robust classifier with adversarial examples...\")\n",
        "for i in range(100):\n",
        "    idx = np.random.randint(len(X_clean))\n",
        "    robust_model.adversarial_train_step(X_clean[idx], y_clean[idx])\n",
        "\n",
        "print(\"✓ Robust classifier trained!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defense 2: Input Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def detect_adversarial(x: np.ndarray, reference_samples: np.ndarray, \n",
        "                       threshold: float = 3.0) -> Tuple[bool, float]:\n",
        "    \"\"\"\n",
        "    Detect adversarial examples using statistical analysis\n",
        "    \n",
        "    Args:\n",
        "        x: Input to check\n",
        "        reference_samples: Clean reference samples\n",
        "        threshold: Detection threshold (in standard deviations)\n",
        "    \n",
        "    Returns:\n",
        "        (is_adversarial, anomaly_score)\n",
        "    \"\"\"\n",
        "    # Compute statistics of reference samples\n",
        "    mean = np.mean(reference_samples, axis=0)\n",
        "    std = np.std(reference_samples, axis=0) + 1e-8\n",
        "    \n",
        "    # Compute z-score\n",
        "    z_scores = np.abs((x - mean) / std)\n",
        "    anomaly_score = np.mean(z_scores)\n",
        "    \n",
        "    is_adversarial = anomaly_score > threshold\n",
        "    \n",
        "    return is_adversarial, anomaly_score\n",
        "\n",
        "# Test detection\n",
        "is_adv_clean, score_clean = detect_adversarial(sample_image, X_clean[:100])\n",
        "is_adv_attack, score_attack = detect_adversarial(adversarial_image, X_clean[:100])\n",
        "\n",
        "print(\"Adversarial Detection Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Clean sample - Anomaly score: {score_clean:.3f}, Detected: {is_adv_clean}\")\n",
        "print(f\"Adversarial sample - Anomaly score: {score_attack:.3f}, Detected: {is_adv_attack}\")\n",
        "\n",
        "if is_adv_attack and not is_adv_clean:\n",
        "    print(\"\\n✓ Detector successfully identified adversarial example!\")\n",
        "elif is_adv_clean:\n",
        "    print(\"\\n⚠ False positive - clean sample flagged as adversarial\")\n",
        "else:\n",
        "    print(\"\\n✗ Failed to detect adversarial example\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defense 3: Ensemble Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class EnsembleDefense:\n",
        "    \"\"\"Ensemble of models for robust prediction\"\"\"\n",
        "    \n",
        "    def __init__(self, num_models: int = 5):\n",
        "        self.models = [SimpleClassifier() for _ in range(num_models)]\n",
        "    \n",
        "    def predict(self, x: np.ndarray) -> Tuple[int, float]:\n",
        "        \"\"\"Predict using ensemble voting\"\"\"\n",
        "        predictions = [model.predict(x) for model in self.models]\n",
        "        \n",
        "        # Majority voting\n",
        "        counts = np.bincount(predictions, minlength=10)\n",
        "        prediction = np.argmax(counts)\n",
        "        confidence = counts[prediction] / len(self.models)\n",
        "        \n",
        "        return prediction, confidence\n",
        "    \n",
        "    def detect_disagreement(self, x: np.ndarray, threshold: float = 0.3) -> bool:\n",
        "        \"\"\"Detect if models disagree (potential adversarial)\"\"\"\n",
        "        predictions = [model.predict(x) for model in self.models]\n",
        "        unique_predictions = len(set(predictions))\n",
        "        disagreement_rate = unique_predictions / len(self.models)\n",
        "        \n",
        "        return disagreement_rate > threshold\n",
        "\n",
        "# Create ensemble\n",
        "ensemble = EnsembleDefense(num_models=5)\n",
        "\n",
        "# Test on clean and adversarial examples\n",
        "clean_pred, clean_conf = ensemble.predict(sample_image)\n",
        "adv_pred, adv_conf = ensemble.predict(adversarial_image)\n",
        "\n",
        "clean_disagree = ensemble.detect_disagreement(sample_image)\n",
        "adv_disagree = ensemble.detect_disagreement(adversarial_image)\n",
        "\n",
        "print(\"Ensemble Defense Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Clean sample - Prediction: {clean_pred}, Confidence: {clean_conf:.1%}, Disagreement: {clean_disagree}\")\n",
        "print(f\"Adversarial sample - Prediction: {adv_pred}, Confidence: {adv_conf:.1%}, Disagreement: {adv_disagree}\")\n",
        "\n",
        "if adv_disagree:\n",
        "    print(\"\\n✓ Ensemble detected suspicious input through model disagreement!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6: Challenge Exercises\n",
        "\n",
        "### Challenge 1: Implement Carlini-Wagner (C&W) Attack\n",
        "Implement a more sophisticated attack that minimizes perturbation while ensuring misclassification:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def carlini_wagner_attack(model: SimpleClassifier, x: np.ndarray, \n",
        "                         target_class: int, c: float = 1.0, \n",
        "                         num_iter: int = 100) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Carlini-Wagner L2 attack (simplified)\n",
        "    \n",
        "    TODO: Implement the C&W attack\n",
        "    Hints:\n",
        "    - Minimize: ||perturbation||^2 + c * loss\n",
        "    - Use optimization to find minimal perturbation\n",
        "    - Ensure adversarial example is misclassified\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# Test your implementation\n",
        "# cw_adversarial = carlini_wagner_attack(model, sample_image, target_class)\n",
        "# Evaluate and compare with FGSM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 2: Implement Defense Distillation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class DistilledClassifier(SimpleClassifier):\n",
        "    \"\"\"\n",
        "    Classifier trained using defensive distillation\n",
        "    \n",
        "    TODO: Implement defensive distillation\n",
        "    Hints:\n",
        "    - Train on soft labels from teacher model\n",
        "    - Use temperature scaling\n",
        "    - Smooth the decision boundaries\n",
        "    \"\"\"\n",
        "    \n",
        "    def distill_from(self, teacher_model: SimpleClassifier, \n",
        "                     X: np.ndarray, temperature: float = 10.0):\n",
        "        \"\"\"Learn from teacher model using distillation\"\"\"\n",
        "        pass\n",
        "\n",
        "# Test your implementation\n",
        "# distilled = DistilledClassifier()\n",
        "# distilled.distill_from(model, X_clean)\n",
        "# Test robustness against FGSM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Challenge 3: Implement Backdoor Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def detect_backdoor(model: SimpleClassifier, X: np.ndarray, \n",
        "                   y: np.ndarray, threshold: float = 0.9) -> Tuple[bool, List[int]]:\n",
        "    \"\"\"\n",
        "    Detect if a model has been backdoored\n",
        "    \n",
        "    TODO: Implement backdoor detection\n",
        "    Hints:\n",
        "    - Look for unusual activation patterns\n",
        "    - Check for triggers that cause consistent misclassification\n",
        "    - Analyze model behavior on modified inputs\n",
        "    \n",
        "    Returns:\n",
        "        (is_backdoored, suspicious_indices)\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# Test on poisoned dataset\n",
        "# is_backdoored, suspicious = detect_backdoor(model, X_poisoned, y_poisoned)\n",
        "# Compare with actual poison_indices from dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary & Key Takeaways\n",
        "\n",
        "In this lab, you learned:\n",
        "\n",
        "1. **Adversarial Examples**: Small perturbations can fool ML models\n",
        "2. **Attack Methods**:\n",
        "   - FGSM: Fast, single-step attack\n",
        "   - PGD: Iterative, more powerful attack\n",
        "   - Data Poisoning: Corrupting training data\n",
        "   - Backdoor Attacks: Hidden triggers in models\n",
        "\n",
        "3. **Defense Strategies**:\n",
        "   - Adversarial training: Train on adversarial examples\n",
        "   - Input validation: Detect anomalous inputs\n",
        "   - Ensemble methods: Use multiple models\n",
        "   - Defensive distillation: Smooth decision boundaries\n",
        "\n",
        "### Best Practices\n",
        "- Always validate inputs before feeding to ML models\n",
        "- Use multiple defense layers\n",
        "- Monitor model behavior in production\n",
        "- Regularly test for adversarial robustness\n",
        "- Be aware of training data integrity\n",
        "\n",
        "### Real-World Impact\n",
        "- Autonomous vehicles: Adversarial stop signs\n",
        "- Face recognition: Fooling authentication systems\n",
        "- Malware detection: Evading security systems\n",
        "- Content moderation: Bypassing filters\n",
        "\n",
        "### Further Reading\n",
        "- [CleverHans Library](https://github.com/cleverhans-lab/cleverhans)\n",
        "- [Adversarial Robustness Toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)\n",
        "- [RobustBench Benchmark](https://robustbench.github.io/)\n",
        "\n",
        "---\n",
        "\n",
        "**HackLearn Pro** - Learn by doing, secure by design.\n"
      ]
    }
  ]
}
